{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '/home/yaosy/Diskb/projects/blitznet_instance_segment/Datasets/coco-seg-train2014-00000-of-00001'\n",
    "raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_features = {\n",
    "    'image/encoded': tf.FixedLenFeature(\n",
    "        (), tf.string, default_value=''),\n",
    "    'image/format': tf.FixedLenFeature(\n",
    "        (), tf.string, default_value='JPEG'),\n",
    "    'image/segmentation/encoded': tf.FixedLenFeature(\n",
    "        (), tf.string, default_value=''),\n",
    "    'image/segmentation/format': tf.FixedLenFeature(\n",
    "        (), tf.string, default_value='RAW'),\n",
    "    'image/object/bbox/xmin': tf.VarLenFeature(\n",
    "        dtype=tf.float32),\n",
    "    'image/object/bbox/ymin': tf.VarLenFeature(\n",
    "        dtype=tf.float32),\n",
    "    'image/object/bbox/xmax': tf.VarLenFeature(\n",
    "        dtype=tf.float32),\n",
    "    'image/object/bbox/ymax': tf.VarLenFeature(\n",
    "        dtype=tf.float32),\n",
    "    'image/object/class/label': tf.VarLenFeature(\n",
    "        dtype=tf.int64),\n",
    "    'image/object/difficulty': tf.VarLenFeature(\n",
    "        dtype=tf.int64),\n",
    "    'image/height': tf.FixedLenFeature(\n",
    "        [], tf.int64, default_value=0),\n",
    "    'image/width': tf.FixedLenFeature(\n",
    "        [], tf.int64, default_value=0),\n",
    "    'image/instance': tf.VarLenFeature(\n",
    "        dtype=tf.int64),\n",
    "    'image/num_instances': tf.FixedLenFeature(\n",
    "        [], tf.int64, default_value=0),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.parse_single_example(example_proto, keys_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset = raw_dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: {image/object/class/label: (?,), image/num_instances: (), image/object/bbox/ymin: (?,), image/encoded: (), image/format: (), image/object/bbox/xmax: (?,), image/object/bbox/xmin: (?,), image/object/difficulty: (?,), image/segmentation/encoded: (), image/segmentation/format: (), image/object/bbox/ymax: (?,), image/height: (), image/instance: (?,), image/width: ()}, types: {image/object/class/label: tf.int64, image/num_instances: tf.int64, image/object/bbox/ymin: tf.float32, image/encoded: tf.string, image/format: tf.string, image/object/bbox/xmax: tf.float32, image/object/bbox/xmin: tf.float32, image/object/difficulty: tf.int64, image/segmentation/encoded: tf.string, image/segmentation/format: tf.string, image/object/bbox/ymax: tf.float32, image/height: tf.int64, image/instance: tf.int64, image/width: tf.int64}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = parsed_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in parsed_dataset.take(4):\n",
    "    t_n = t['image/num_instances']\n",
    "    t_w = t['image/width']\n",
    "    t_h = t['image/height']\n",
    "    b_x1 = t['image/object/bbox/xmin']\n",
    "    b_y1 = t['image/object/bbox/ymin']\n",
    "    b_x2 = t['image/object/bbox/xmax']\n",
    "    b_y2 = t['image/object/bbox/ymax']\n",
    "    t_cl = t['image/object/class/label']\n",
    "    t_ins = t['image/instance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ins_temp = t_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ins = t_ins_temp.values\n",
    "\n",
    "t_ins = tf.reshape(t_ins, (20, t_h.numpy(), t_w.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ins_np = t_ins.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 700, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_ins_np.shape[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ins = tf.transpose(t_ins, [1, 2, 0])\n",
    "t_ins_np = t_ins.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1000x700 at 0x7F682C68CB70>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = t_ins_np[:, :, 0]\n",
    "x, y = np.where(test_img > 0)\n",
    "test_img[x, y] = 255\n",
    "test_img = test_img.astype(np.uint8)\n",
    "Image.fromarray(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1000x700 at 0x7F683B758BA8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = t_ins_np[0, :, :]\n",
    "x, y = np.where(test_img > 0)\n",
    "test_img[x, y] = 255\n",
    "test_img = test_img.astype(np.uint8)\n",
    "Image.fromarray(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1111, shape=(4, 4), dtype=float32, numpy=\n",
       "array([[ 0.26671898,  0.09058242,  0.58814752,  0.42158243],\n",
       "       [ 0.5624333 ,  0.28358242,  0.8824333 ,  0.57458246],\n",
       "       [ 0.14529043,  0.50258243,  0.54529047,  0.78658247],\n",
       "       [ 0.49957615,  0.65258241,  0.8710047 ,  0.91358244]], dtype=float32)>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack((b_y1.values, b_x1.values, b_y2.values, b_x2.values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.ima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.boundingRect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_np3 = t_ins_np[5] > 0\n",
    "ins_np3 = ins_np3.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAK8CAAAAABZoMptAAACvUlEQVR4nO3BAQEAAACCIP+vbkhAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8GsbIAAYYJSIcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1000x700 at 0x7F05F87C1940>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(ins_np3, 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1173, shape=(3,), dtype=int64, numpy=array([15, 15, 15])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_cl.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_np = t1.values.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 15, 15, ..., 15, 15, 15])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_np[t1_np != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([1,2, tf.constant(2)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Ranks of all input tensors should match: shape[0] = [2] vs. shape[1] = [] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ae3752a868ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m       return concat_v2_eager_fallback(\n\u001b[0;32m--> 971\u001b[0;31m           values, axis, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    972\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2_eager_fallback\u001b[0;34m(values, axis, name, ctx)\u001b[0m\n\u001b[1;32m    993\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"N\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tidx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m   _result = _execute.execute(b\"ConcatV2\", 1, inputs=_inputs_flat,\n\u001b[0;32m--> 995\u001b[0;31m                              attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m    996\u001b[0m   _execute.record_gradient(\n\u001b[1;32m    997\u001b[0m       \"ConcatV2\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Ranks of all input tensors should match: shape[0] = [2] vs. shape[1] = [] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "tf.concat([tf.constant([1, 2]), tf.constant(2)], axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
